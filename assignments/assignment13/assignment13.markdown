---
title: Assignment 13 - Generative Pre-Trained Transformers (GPTs) Part 2
toc_sticky: true 
toc_h_max: 1
layout: problemset
---

# Learning Objectives

{% capture content %}
* Generalize from single-headed to multi-headed attention
* Interleave attention and MLPs to create a transformer
* Understand that optimization performance can be improved through residual connections and normalization
* Think through some ethical issues around selecting sources for a text dataset
{% endcapture %}
{% include learning_objectives.html content=content %}

# Building on Single-headed Self-attention

Let's take a minute to reflect on where we ended our last assignment.

TODO: follow Karpathy for the most part.  Probably mostly we are understanding rather than writing code for this part.

TODO: Ask class to suggest data sources and discuss ethical considerations.  We can come up with a high-level framing like creating a system to allow new students to learn about Olin.   Maybe (probably) we don't actually train it (although it might be cool, although a lot of work).  Could be a good discussion starter.