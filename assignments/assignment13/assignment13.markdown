---
title: Assignment 13 - Generative Pre-Trained Transformers (GPTs) Part 2
toc_sticky: true 
toc_h_max: 1
layout: problemset
---

# Learning Objectives

{% capture content %}
* Combine attention and an MLP to reduce loss
* Encapsulate pytorch modules to allow scaling up
* Improving optimization performance with residual connections and normalization
* Ethical issues in training text completion modules
{% endcapture %}
{% include learning_objectives.html content=content %}

TODO: follow Karpathy for the most part.  Probably mostly we are understanding rather than writing code for this part.

TODO: Ask class to suggest data sources and discuss ethical considerations.  We can come up with a high-level framing like creating a system to allow new students to learn about Olin.   Maybe (probably) we don't actually train it (although it might be cool, although a lot of work).  Could be a good discussion starter.