<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<script>
class AnchorNoProxy extends HTMLElement {
  constructor() {
    super();
    this.attachShadow({ mode: "open" });
    this._$a = null;
  }
  connectedCallback() {
    const href = this.getAttribute("href") || "#";
    if (this.dataset.hasOwnProperty('canvas')) {
        const canvasURL = this.dataset.canvas;
        this.shadowRoot.innerHTML = `<style>a:hover, a:active { outline: 0; }\na { color: #5197ad; }\na:visited { color: #5197ad; }\na:hover { color: #266477; outline: 0; }</style><a href="${href}" data-canvas="${canvasURL}"><slot></slot></a>`;
    } else {
        this.shadowRoot.innerHTML = `<style>a:hover, a:active { outline: 0; }\na { color: #5197ad; }\na:visited { color: #5197ad; }\na:hover { color: #266477; outline: 0; }</style><a href="${href}"><slot></slot></a>`;
    }
    this._$a = this.shadowRoot.querySelector("a");
    this._$a.addEventListener("click", e => {
      var url = this.getAttribute('href');
      e.preventDefault();
      if (document.referrer.startsWith('https://lms.hypothes.is') && this.dataset.hasOwnProperty('canvas')) {
        // get rid of proxy if it was added
        var n = this.dataset.canvas.search('https://olin.instructure.com');
        window.open(this.dataset.canvas.substring(n), '_blank');
      } else {
        window.open(url, '_blank');
      }
    });
  }
  static get observedAttributes() { return ["href"]; }
  attributeChangedCallback(name, oldValue, newValue) {
    if (oldValue !== newValue) {
      if (this._$a === null) return;
      this._$a.setAttribute("href", newValue);
    }
  }
}

customElements.define("a-no-proxy", AnchorNoProxy);

class NoProxy extends HTMLAnchorElement {
  connectedCallback() {
    this.addEventListener("click", e => {
      e.preventDefault();
      if (document.referrer.startsWith('https://lms.hypothes.is') && this.dataset.hasOwnProperty('canvas')) {
        // get rid of proxy if it was added
        var n = this.dataset.canvas.search('https://olin.instructure.com');
      	window.open(this.dataset.canvas.substring(n), '_blank');
      } else {
      	window.open(this.href, '_blank');
      }
    });
  }
}

customElements.define("no-proxy", NoProxy, { extends: "a" });

class ConfirmLink extends HTMLAnchorElement {
  connectedCallback() {
    this.addEventListener("click", e => {
      const result = confirm(`Are you sure you want to go to '${this.href}'?`);
      if (!result) e.preventDefault();
    });
  }
}

customElements.define("confirm-link", ConfirmLink, { extends: "a" });

</script>

<!-- begin _includes/seo.html --><title>Assignment 5 - Machine Learning Fall 2024 @ Olin College</title>
<meta name="description" content="">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Machine Learning Fall 2024 @ Olin College">
<meta property="og:title" content="Assignment 5">
<meta property="og:url" content="/assignments/assignment05/assignment05.html">













<link rel="canonical" href="/assignments/assignment05/assignment05.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Machine Learning Fall 2024 @ Olin College Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

<script type="text/javascript">
document.addEventListener('DOMContentLoaded', function() {
    const urlParams = new URLSearchParams(window.location.search);
    const showSolutions = urlParams.get('showSolutions');
    const showAllSolutions = urlParams.get('showAllSolutions');
    const divs = document.querySelectorAll('button.togglebutton');

    if (showSolutions == 'true') {
        // Loop through the selected divs and manipulate them
        divs.forEach(div => {
            div.removeAttribute('hidden');
        });
    }

    const solutionDivs = document.querySelectorAll('[id^=solution]');
    const subpartSolutionDivs = document.querySelectorAll('[id^=subpartsolution]');

    if (showAllSolutions == 'true') {
        solutionDivs.forEach(div => {
            console.log('test')
            div.style.display = 'block';
        });

        subpartSolutionDivs.forEach(div => {
            console.log('test')
            div.style.display = 'block';
        });
    }
});
</script>

<script type="text/javascript">
function HideShowElement(divID) {
    const x = document.getElementById(divID);
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>
<style>
:root {
    --box-bg-color: #fff; /* Default background color */
}

.homework-box {
    background-color: var(--box-bg-color);
    border: 2px solid #ccc;
    padding: 15px;
    border-radius: 10px;
    margin-bottom: 20px;
    width: 300px;
}

.homework-header {
    display: flex;
    align-items: center;
    margin-bottom: 10px;
    position: relative;
}

.homework-icon {
    width: 40px;
    height: 40px;
    margin-right: 10px;
}

/* Handle missing or empty src attribute */
.homework-icon[src=""],
.homework-icon:not([src]) {
    content: url('https://upload.wikimedia.org/wikipedia/commons/a/ab/Games_for_Learning_%2827470%29_-_The_Noun_Project.svg');
    width: 40px;
    height: 40px;
    margin-right: 10px;
}

.homework-title {
    margin: 0;
    font-size: 18px;
    font-weight: bold;
}

.homework-content {
    font-size: 16px;
    color: #333;
}
</style>

<style>
.solution {
    display: none; /* Hide solutions by default */
    background-color: #f9f9f9;
    padding: 10px;
    border-left: 4px solid #007bff;
    margin-top: 10px;
}

.toggle-button {
    background-color: #007bff;
    color: #fff;
    padding: 5px 10px;
    border: none;
    border-radius: 5px;
    cursor: pointer;
    margin-top: 10px;
    display: inline-block;
}

.toggle-button.hide-solution {
    background-color: #dc3545; /* Red background for hide button */
}

img.mermaid {
     max-width:500px;
     text-align: center;
}

</style>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.7/tex-mml-chtml.js"></script>
</head>
<body>
<div style="display:none;">
$
\newcommand{\mlvec}[1]{\mathbf{#1}}
\newcommand{\mlmat}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max\,}
\DeclareMathOperator*{\argmin}{arg\,min\,}
$
</div>

  

  
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="https://qeacourse.github.io/RoboNinjaWarrior/website_graphics/olinlogo.png" alt=""></a>
        
        <a class="site-title" href="/">
          Machine Learning Fall 2024 @ Olin College
          
        </a>
        <ul class="visible-links"></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Assignment 5">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Assignment 5
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title">
<i class="fas fa-file-alt"></i> On this page</h4></header>
	      
                <ul class="toc__menu">
  <li><a href="#learning-objectives">Learning Objectives</a></li>
  <li><a href="#the-logistic-regression-model">The Logistic Regression Model</a></li>
  <li><a href="#deriving-the-logistic-regression-learning-rule">Deriving the Logistic Regression Learning Rule</a></li>
  <li><a href="#dataflow-diagrams-and-foundations-of-micrograd">Dataflow Diagrams and Foundations of Micrograd</a></li>
</ul>
	      	
            </nav>
          </aside>
        
        <h1 id="learning-objectives">Learning Objectives</h1>

<div class="tip" style="
    border-left: 6px solid #000000;
    margin: 2em 2em 2em 2em;">
	<div style="background-color: #C6EBD5;
	                column-gap: 1rem;
					display: flex;
					padding: 1em 1em 1em 1em;">
	<div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #000000;">
        <i class="fas fa-brain"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Learning Objectives</div>
	</div>
	<div style="padding: 1em 1em 1em 1em;">
	   
<ul>
  <li>Learn about the logistic regression algorithm.</li>
  <li>Learn about gradient descent for optimization.</li>
  <li>Build the foundational undertanding we will need to implement the micrograd algorithm.</li>
</ul>

	</div>
</div>

<p>This builds on:</p>
<ul>
  <li>
<a href="/assignments/assignment03/assignment03?showAllSolutions=true#supervised-learning-problem-setup">Supervised learning problem framing</a>.</li>
  <li>Calculating gradients.</li>
  <li><a href="/assignments/assignment04/assignment04?showAllSolutions=true#probability-and-the-log-loss">Log loss</a></li>
</ul>

<h1 id="the-logistic-regression-model">The Logistic Regression Model</h1>

<p>In class we went over a simple application of logistic regression to the Titanic Dataset.  So you have it handy, here is a link to the <a href="https://colab.research.google.com/drive/1xpGvY-kg7-HOC7_To0nMZIOOHQ_Yxd89?usp=sharing">notebook from class</a>.  You don’t have to do anything with this notebook for this assignment, but we wanted you to have it handy.</p>

<div class="tip" style="
    border-left: 6px solid #000000;
    margin: 2em 2em 2em 2em;">
	<div style="background-color: #FDFD96;
	                column-gap: 1rem;
					display: flex;
					padding: 1em 1em 1em 1em;">
	<div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #000000;">
        <i class="fa fa-exclamation-triangle"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Notice</div>
	</div>
	<div style="padding: 1em 1em 1em 1em;">
	   
<h2 id="recall">Recall</h2>

<p>In the last assignment, you were introduced to the idea of binary classification, which based on some input $\mlvec{x}$ has a corresponding output $y$ that is $y= 0$ or $y= 1$. In logistic regression, this model, $\hat{f}$, instead of spitting out either 0 or 1, outputs a confidence that the input $\mlvec{x}$ has an output $y= 1$.  In other words, rather than giving us its best guess (0 or 1), the classifier indicates to us its degree of certainty regarding its prediction as a probability.</p>

<p>We also explored three possible loss functions for a model that outputs a probability $p$ when supplied with an input $\mlvec{x}$ (i.e., $\hat{f}(\mlvec{x})=p$). The loss function is used to quantify how bad a prediction $p$ is given the actual output $y$ (for binary classification the output is either $0$ or $1$).</p>

<ol>
  <li>
<strong>0-1 loss:</strong> This is an all-or-nothing approach. If the prediction is correct, the loss is zero; if the prediction is incorrect, the loss is 1. This does not take into account the level certainty expressed by the probability (the model gets the same loss if $y = 1$ and it predicted $p = 0.51$ or $p = 1$).</li>
  <li>
<strong>squared loss:</strong> For squared loss we compute the difference between the outcome and $p$ and square it to arrive at the loss.  For example, if $y = 1$ and the model predicts $p = 0.51$, the loss is $(1 - 0.51)^2$.  If instead $y = 0$, the loss is $(0 - 0.51)^2$.</li>
  <li>
<strong>log loss:</strong> The log loss also penalizes based on the difference between the outcome, $y_i$, and the predicted probabilty, $p_i$, using the formula below.</li>
</ol>
<div id="eqlogloss">
\begin{align}
 \mbox{logloss} = -\frac{1}{N}\sum_{i=1}^n \Big( y_i \ln (p_i) + (1-y_i) \ln (1 - p_i) \Big )\tag{1}
\end{align}
</div>

<p>Since $y_i$ is always 0 or 1, we will essentially switch between the two chunks of this equation based on the true value of $y_i$. As the predicted probability, $p_i$ (which is constrained between 0 an 1) gets farther from $y_i$, the log-loss value increases.</p>


	</div>
</div>

<p>Now that you have refreshed on how probabilities can be used as a way of quantifying confidence in predictions, you are ready to learn about the logistic regression algorithm.</p>

<p>As always, we assume we are given a training set of inputs and outputs.  As in linear regression we will assume that each of our inputs is a $d$-dimensional vector $\mathbf{x_i}$ and since we are dealing with binary classification, the outputs, $y_i$, will be binary numbers (indicating whether the input belongs to class 0 or 1).  Our hypothesis functions, $\hat{f}$, output the probability that a given input has an output of 1.  What’s cool is that we can borrow a lot of what we did in the last couple of assignments when we learned about linear regression.  In fact, all we’re going to do in order to make sure that the output of $\hat{f}$ is between 0 and 1 is pass $\mlvec{w}^\top \mlvec{x}$ through a function that “squashes” its input so that it outputs a value between 0 and 1.  This idea is shown graphically in thie following figure.</p>

<figure id="figure1">
    <div style="text-align: center;">
        <img src="figures/linearandlogistic.png" alt="a schematic of a neural network is used to represent linear and logistic regression.  Circles represent nodes, which are connected to other nodes using arrows. Logistic regression looks like linear regression followed by a sigmoid function." style="max-width: 50%">
    </div>
    <figcaption style="font-size: large">
        Figure 1: <p>Graphical representation of both linear and logistic regression.  The key difference is the application of the squashing function shown in yellow. <a href="https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24">Original Source - Towards Data Science</a></p>

    </figcaption>
</figure>

<p>To make this intuition concrete, we define each $\hat{f}$ as having the following form (note: this equation looks daunting. We have some tips for interpreting it below).</p>

<div id="logistichypothesis">
\begin{align}
\hat{f}(\mathbf{x}) &amp;= \mbox{probability that output, $y$, is 1} \nonumber  \\  
&amp;=\frac{1}{1 + e^{-\mlvec{w}^\top \mathbf{x}}} \tag{2}
\end{align}
</div>

<p>Here are a few things to notice about this equation:</p>
<ol>
  <li>The weight vector that we saw in linear regression, $\mlvec{w}$, has made a comeback. We are using the dot product between $\mlvec{x}$ and $\mlvec{w}$ (which creates a weighted sum of the $x_i$’s), just as we did in linear regression!</li>
  <li>As indicated in <a href="#figure1">Figure 1</a>, the dot product $\mlvec{w}^\top \mlvec{x}$ has been passed through a squashing function known as the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>.  The graph of $\sigma(u) = \frac{1}{1+e^{-u}}$ is shown in <a href="#figure2">Figure 2</a>.  $\sigma( \mlvec{w}^\top \mlvec{x})$ is exactly what we have in $$ \hat{f}(\mathbf{x}) =\frac{1}{1 + e^{-\mlvec{w}^\top \mathbf{x}}}$$</li>
</ol>

<figure id="figure2">
    <div style="text-align: center;">
        <img src="figures/Logistic-curve.png" alt="a sigmoid function that is flat, curves up, and then flattens out again" style="max-width: 50%">
    </div>
    <figcaption style="font-size: large">
        Figure 2: <p>A graph of the sigmoid function $\frac{1}{1+e^{-x}}$.</p>

    </figcaption>
</figure>

<h1 id="deriving-the-logistic-regression-learning-rule">Deriving the Logistic Regression Learning Rule</h1>

<p>Now we will formalize the logistic regression problem and derive a learning rule to solve it (i.e., compute the optimal weights). The formalization of logistic regression will combine <a href="#logistichypothesis">Equation 2</a> with the selection of $\ell$ to be log loss (<a href="#eqlogloss">Equation 1</a>).  This choice of $\ell$ results in the following objective function (this is a straightforward substitution.  there’s nothing too tricky going on here).</p>

<p>
\begin{align}
\mlvec{w}^\star &amp;= \argmin_{\mlvec{w}} \sum_{i=1}^n \Big ( - y_i \ln \sigma(\mlvec{w}^\top \mlvec{x_i}) - (1-y_i) \ln (1 - \sigma(\mlvec{w}^\top \mlvec{x_i}) ) \Big)  \\  
&amp;= \argmin_{\mlvec{w}} \sum_{i=1}^n \Bigg (  - y_i \ln \left ( \frac{1}{1+e^{-\mlvec{w}^\top \mlvec{x_i}}} \right) - (1-y_i) \ln  \left (1 - \frac{1}{1+e^{-\mlvec{w}^\top \mlvec{x_i}}} \right ) \Bigg) &amp;\mbox{expanded out if you prefer this form} \label{eq:objective}
\end{align}
</p>

<p>While this looks a bit intense, since $y_i$ is either 0 or 1, the multiplication of the expressions in the summation by either $y_i$ or $1-y_i$ are essentially acting like a switch—depending on the value of $y_i$ we either get one term or the other.  Our typical recipe for finding $\mlvec{w}^\star$ has been to take the gradient of the expression inside the $\argmin$, set it to $0$, and solve for $\mlvec{w}^\star$ (which will be a critical point and hopefully a minimum).  The last two steps will be a bit different for reasons that will become clear soon, but we will need to find the gradient.  We will focus on finding the gradient in the next couple of parts.</p>

<h2 id="useful-properties-of-the-sigmoid-function">Useful Properties of the Sigmoid Function</h2>

<p>The equation for $\mlvec{w}^\star$ above looks really hairy! We see that in order to compute the gradient we will have to compute the gradient of $\mathbf{x}^\top \mlvec{w}$ with respect to $\mlvec{w}$ (we just wrapped our minds around this last assignment).  Additionally, we will have to take into account how the application of the sigmoid function and the log function changes this gradient.  In this section we’ll learn some properties for manipulating the sigmoid function and computing its derivative.</p>

<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 1</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p>The sigmoid function, $\sigma$, is defined as</p>

<p>$$
\begin{align}
\sigma(x) &amp;= \frac{1}{1+e^{-x}}
\end{align}
$$</p>

<p style="font-size: x-large; font-weight: 700;">Part A</p>

<p>Show that $\sigma(-x) = 1 - \sigma(x)$.</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-32")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-32" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>
\begin{align}
\sigma(-x) &amp;= \frac{1}{1+e^{x}} \\
&amp;= \frac{e^{-x}}{e^{-x} + 1}~~\mbox{multiply by top and bottom by $e^{-x}$} \\
 \sigma(-x)  - 1&amp;= \ \frac{e^{-x}}{e^{-x} + 1} - \frac{1 + e^{-x}}{1 + e^{-x}} ~~\mbox{subtract $-1$ on both sides} \\
 &amp;= \frac{-1}{1+e^{-x}} \\
 &amp;= -\sigma(x) \\
 \sigma(-x) &amp;= 1 - \sigma(x)
\end{align}
</p>


</div>
</div>

<p style="font-size: x-large; font-weight: 700;">Part B</p>

<p>Show that the derivative of the logistic function $\frac{d}{dx} \sigma(x) = \sigma(x) (1 - \sigma(x))$</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-33")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-33" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>Two solutions for the price of 1!</p>

<p>Solution 1:</p>
<p>
\begin{align}
\frac{d}{dx} \sigma(x)  &amp;= e^{-x} \sigma(x)^2 &amp;\mbox{apply quotient rule} \\
&amp;= \sigma(x) \left ( \frac{e^{-x}}{1 + e^{-x}} \right) &amp;\mbox{expand out one of the $\sigma(x)$'s}\\
&amp;= \sigma(x) \left ( \frac{1}{e^{x} + 1} \right) &amp; \mbox{multiply top and bottom by $e^{x}$}\\
&amp;=  \sigma(x) (  \sigma(-x)) &amp;\mbox{substitute for $\sigma(-x)$} \\
&amp;=  \sigma(x) (1 -  \sigma(x) ) &amp;\mbox{apply $\sigma(-x)=1-\sigma(x)$}
\end{align}
</p>

<p>Solution 2:</p>
<p>
\begin{align}
\frac{d}{dx} \sigma(x)  &amp;=\frac{e^{-x}}{(1+e^{-x} )^2} &amp; \mbox{apply quotient rule} \\
&amp;= \frac{e^{-x}}{1+2e^{-x} + e^{-2x}} &amp; \mbox{expand the bottom}\\
&amp;= \frac{1}{e^{x}+2 + e^{-x}} &amp; \mbox{multiply top and bottom by $e^{x}$}\\
&amp;= \frac{1}{(1+e^{x})(1+e^{-x})} &amp; \mbox{factor} \\
&amp;= \sigma(x)\sigma(-x) &amp; \mbox{decompose using definition of $\sigma(x)$}\\
&amp;= \sigma(x)(1-\sigma(x)) &amp;\mbox{apply $\sigma(-x)=1-\sigma(x)$}
\end{align}
</p>


</div>
</div>


</div>
</div>

<h2 id="chain-rule-for-gradients">Chain Rule for Gradients</h2>
<p>We now know how to take derivatives of each of the major pieces of the logistic regression loss function.  What we need is a way to put these derivatives together.  You probably remember that in the case of single variable calculus you have just such a tool.  This tool is known as the chain rule.  The chain rule tells us how to compute the derivative of the composition of two single variable functions $f$ and $g$.</p>

<p>
\begin{align}
h(x)&amp;= g(f(x))&amp;\mbox{h(x) is the composition of $f$ with $g$} \nonumber \\
h'(x) &amp;= g'(f(x))f'(x)&amp;\mbox{this is the chain rule!}
\end{align}
</p>

<p>Suppose that instead of the input being a scalar $x$, the input is now a vector, $\mlvec{w}$.  In this case $h$ takes a vector input and returns a scalar, $f$ takes a vector input and returns a scalar, and $g$ takes a scalar input and returns a scalar.</p>

<p>
\begin{align}
h(\mlvec{w}) &amp;= g(f(\mlvec{w}))&amp;\mbox{h($\mlvec{w}$) is the composition of $f$ with $g$} \nonumber \\
\nabla h(\mlvec{w}) &amp;= g'(f(\mlvec{w})) \nabla f(\mlvec{w}) &amp; \mbox{this is the multivariable chain rule}
\end{align}
</p>

<div id="chainrule">


<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 2</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p style="font-size: x-large; font-weight: 700;">Part A</p>

<p>Suppose $h(x) = \sin(x^2)$, compute $h’(x)$ (x is a scalar so you can apply the single-variable chain rule).</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-34")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-34" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>Applying the chain rule gives
$$
\begin{align}
h’(x) &amp;= cos(x^2) 2x
\end{align}
$$</p>

</div>
</div>

<p style="font-size: x-large; font-weight: 700;">Part B</p>

<p>Define $h(\mlvec{v}) = (\mlvec{c}^\top \mlvec{v})^2$.  Compute $\nabla_{\mlvec{v}} h(\mlvec{v})$ (the gradient of the function with respect to $\mlvec{v}$).</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-35")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-35" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>We can see that $h(\mlvec{v}) = g(f(\mlvec{v}))$ with $g(x) = x^2$ and $f(\mlvec{v}) = \mlvec{c}^\top \mlvec{v}$ The gradient can now easily be found by applying the chain rule.</p>

<p>$$
\begin{align}
\nabla h(\mlvec{v}) &amp;= 2(\mlvec{c}^\top \mlvec{v}) \mlvec{c}
\end{align}
$$</p>


</div>
</div>

<p style="font-size: x-large; font-weight: 700;">Part C</p>

<p>Compute the gradient of this expression, which comes from the beginning of the section on deriving the logistic regression learning rule:</p>

<div>
\begin{align}
 \sum_{i=1}^n -y_i \ln \sigma( \mlvec{w}^\top \mlvec{x_i}) - (1-y_i) \ln  \left (1 - \sigma( \mlvec{w}^\top \mlvec{x_i}) \right ) 
\end{align}
</div>

<p>You can either use the chain rule and the identities you learned about sigmoid, or expand everything out and work from that.</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-36")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-36" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>Applying the chain rule gives us</p>

<p>
\begin{align}
 \sum_{i=1}^n -y_i \frac{\nabla \sigma( \mlvec{w}^\top \mlvec{x_i})}{\sigma( \mlvec{w}^\top \mlvec{x_i})} - (1-y_i) \frac{- \nabla \sigma( \mlvec{w}^\top \mlvec{x_i})}{1 - \sigma( \mlvec{w}^\top \mlvec{x_i})}  \enspace .
\end{align}
</p>

<p>Applying the chain rule again gives us</p>
<p>
\begin{align}
&amp; \sum_{i=1}^n -y_i \frac{\sigma( \mlvec{w}^\top \mlvec{x_i})(1-\sigma( \mlvec{w}^\top \mlvec{x_i}))\nabla \mlvec{w}^\top \mlvec{x_i}}{\sigma( \mlvec{w}^\top \mlvec{x_i})} - (1-y_i) \frac{- \sigma( \mlvec{w}^\top \mlvec{x_i})(1-\sigma( \mlvec{w}^\top \mlvec{x_i}))\nabla \mlvec{w}^\top \mlvec{x_i}}{1 - \sigma( \mlvec{w}^\top \mlvec{x_i})} \nonumber \\
 &amp;= \sum_{i=1}^n -y_i (1-\sigma( \mlvec{w}^\top \mlvec{x_i}))\mlvec{x_i} + (1-y_i)  \sigma( \mlvec{w}^\top \mlvec{x_i})) \mlvec{x_i} 
 \end{align}
 </p>

<p>You could certainly stop here, but if you plug in $y=0$ and $y=1$ you’ll find that the expression can be further simplified to:</p>

<p>
 \begin{align}
\sum_{i=1}^n  -(y_i - \sigma(\mlvec{w}^\top \mlvec{x_i})) \mlvec{x_i} \nonumber
 \end{align}
</p>


</div>
</div>


</div>
</div>
</div>

<h2 id="gradient-descent-for-optimization">Gradient Descent for Optimization</h2>

<p>If we were to follow our derivation of linear regression we would set our expression for the gradient to 0 and solve for $\mlvec{w}$.  It turns out this equation will be difficult to solve due to the $\sigma$ function.  Instead, we can use an iterative approach where we start with some initial value for $\mlvec{w}$ (we’ll call the initial value $\mlvec{w^0}$, where the superscript corresponds to the iteration number) and iteratively adjust it by moving down the gradient (the gradient represents the direction of fastest increase for our function, therefore, moving along the negative gradient is the direction where the loss is decreasing the fastest).</p>

<div class="tip" style="
    border-left: 6px solid #000000;
    margin: 2em 2em 2em 2em;">
	<div style="background-color: #FFD1DC;
	                column-gap: 1rem;
					display: flex;
					padding: 1em 1em 1em 1em;">
	<div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #000000;">
        <i class="fas fa-external-link-alt"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">External Resources</div>
	</div>
	<div style="padding: 1em 1em 1em 1em;">
	   
<p>There are tons of great resources that explain gradient descent with both math and compelling visuals.</p>

<ul>
  <li>Recommended: <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">Gradient descent, how neural networks learn - Deep learning, chapter 2, start at 5:20</a>
</li>
  <li><a href="https://medium.com/@viveksingh.heritage/an-introduction-to-gradient-descent-54775b55ba4f">An Introduction to Gradient Descent</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Gradient_descent">The Wikipedia page on Gradient Descent</a></li>
  <li>
<a href="https://www.youtube.com/watch?v=fPSPdTjINi0">Ahmet Sacan’s video on gradient descent</a> (this one has some extra stuff, but it’s pretty clearly explained).</li>
  <li>There are quite a few resources out there, do you have some suggestions? (Share on Slack!)</li>
</ul>


	</div>
</div>

<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 3</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p>To test your understanding of these resources, here are a few diagnostic questions.</p>

<p style="font-size: x-large; font-weight: 700;">Part A</p>

<p>When minimizing a function with gradient descent, which direction should you step along in order to arrive at the next value for your parameters?</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-37")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-37" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>The negative gradient (since we are minimizing)</p>


</div>
</div>

<p style="font-size: x-large; font-weight: 700;">Part B</p>

<p>What is the learning rate and what role does it serve in gradient descent?</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-38")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-38" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>The learning rate controls the size of the step that you take along the negative gradient.</p>

</div>
</div>

<p style="font-size: x-large; font-weight: 700;">Part C</p>

<p>How do you know when an optimization performed using gradient descent has converged?</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-39")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-39" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>There are a few options.  One popular one is to check if the objective function is changing  only a minimal amount each iteration, the algorithm has converged.  You could also look at the magnitude of the gradient (which tells us the slope) to see if it is really small.</p>

</div>
</div>

<p style="font-size: x-large; font-weight: 700;">Part D</p>

<p>True or false: provided you tune the learning rate properly, gradient descent guarantees that you will find the global minimum of a function.</p>

<p><button hidden="true" onclick='HideShowElement("subpartsolution-40")' class="togglebutton">Show / Hide Solution</button></p>

<div id="subpartsolution-40" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>False, the best gradient descent can do, in general, is converge to a local minimum.  If you know that the function you are optimizing has only one minimum, then this would also be the global minimum (this is the case for both linear and logistic regression).</p>

</div>
</div>


</div>
</div>

<p>If we take the logic of gradient descent and apply it to the logistic regression problem, we arrive at the following learning rule.  Given some initial weights $\mlvec{w^0}$, and a learning rate $\eta$, we can iteratively update our weights using the formula below.</p>

<p>We start by applying the results from our <a href="../assignment04/assignment04?showSolutions=true#chainrule">exercise on the chain rule.</a></p>

<p>
\begin{align}
\mlvec{w^{n+1}} &amp;= \mlvec{w^n} - \eta \sum_{i=1}^n  -(y_i - \sigma(\mlvec{w}^\top \mlvec{x_i})) \mlvec{x_i} \\
&amp;=  \mlvec{w^n} + \eta \sum_{i=1}^n  (y_i - \sigma(\mlvec{w}^\top \mlvec{x_i})) \mlvec{x_i}  ~~~\mbox{distribute the negative}
\end{align}
</p>

<p>This beautiful equation turns out to be the recipe for logistic regression.</p>

<div class="tip" style="
    border-left: 6px solid #000000;
    margin: 2em 2em 2em 2em;">
	<div style="background-color: #FDFD96;
	                column-gap: 1rem;
					display: flex;
					padding: 1em 1em 1em 1em;">
	<div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #000000;">
        <i class="fa fa-exclamation-triangle"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Notice</div>
	</div>
	<div style="padding: 1em 1em 1em 1em;">
	   
<p>We won’t be assigning a full implementation of logistic regression from scratch. In future assignments, we will spend more time applying logistic regression and gradient descent.</p>

<p>If it’s helpful for your learning to see a worked example with code now (to help the math make sense), you can optionally check out this <a href="https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24">example of binary classification for admission to college</a>, noting that some of the math notation is slightly different than ours.</p>

<p>You are also welcome to implement logistic regression using gradient descent if it’s helpful for your learning and/or if you already have significant experience with machine learning and want a challenge. This is completely optional, and we assume that most of you will not choose to do this. If you do decide to implement logistic regression using gradient descent, you will need to search for a good learning rate or you may consider implementing some <a href="https://towardsdatascience.com/gradient-descent-algorithms-and-adaptive-learning-rate-adjustment-methods-79c701b086be">strategies for automatically tuning the learning rate</a>.</p>

	</div>
</div>

<!-- # Machine learning for loans and mortgages

In this course, we'll be exploring machine learning from three different perspectives: the theory, the implementation, and the context, impact, and ethics.  -->

<h1 id="dataflow-diagrams-and-foundations-of-micrograd">Dataflow Diagrams and Foundations of Micrograd</h1>

<p>Now that we have derived a learning rule for logistic regression, we are going to look at another way of representing multivariable functions and computing their partial derivatives.  This way of thinking about multivariable functions may seem a little strange at first, but this notion is going to lay the foundation for being able to derive learning rules for a whole range of machine learning models in an automated fashion!!</p>

<p>First, let’s look at a multivariable function defined by the equations below.  We have a single scalar input variable $t$ that affects both input arguments of $f$ (through $x(t)$ and $y(t)$).</p>

<p>
\begin{align}
x &amp;= x(t) \\
y &amp;= y(t) \\
f &amp;= f(x, y) \\
\end{align}
</p>

<p>Let’s represent this system of equations using a data flow diagram (<a href="https://math.libretexts.org/Bookshelves/Calculus/Calculus_(OpenStax)/14%3A_Differentiation_of_Functions_of_Several_Variables/14.05%3A_The_Chain_Rule_for_Multivariable_Functions">in some resources</a> this is called a tree diagram, in which case it is drawn a bit differently).</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoiZmxvd2NoYXJ0IEJUXG5pZDFbXCIkJGYgPSBmKHgseSl-fn5-JCRcIl1cbmlkMltcIiQkeCA9IHgodCl-fiQkXCJdXG5pZDNbXCIkJHkgPSB5KHQpfn4kJFwiXVxuaWQyIC0tPiBpZDFcbmlkMyAtLT4gaWQxXG50IC0tPiBpZDJcbnQgLS0-IGlkMyIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In19"></p>

<p>This diagram represents how data moves from the inputs of a function (in this case $x$ and $y$) to its output (in this case $f$).  If we were to take a chart like this and figure out how to evaluate a function given some inputs, you’d have to make sure you always evaluate the inputs to a block before you try to evaluate the block itself.  For instance, I wouldn’t be able to evaluate the block $f = f(x,y)$ until I’ve evaluated the blocks $x = x(t)$ and $y=y(t)$.  To evaluate a block, you can imagine that the output of a block flows along the arrow into the downstream block, which then processes that input further until it arrives at the output.</p>

<p>Let’s say we want to calculate $\frac{\partial f}{\partial t}$.  We’ve learned about the chain rule for single variable functions, but this case is a bit different.  It turns out that, in this case, we can compute the partial derivative we seek in the following way.</p>

<p>\begin{align}
\frac{\partial f}{\partial t} &amp;= \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} +  \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}
\end{align}</p>

<p>What is this formula saying???  Well it looks awfully like the single variable chain rule in the sense that we are multiplying derivatives together.  The only difference is that we are having to account for the multiple pathways from the input (independent variable) $t$ to the output (dependent variable) $f$.</p>

<p>In the resources below, you will see how we can use our data flow diagram to compute these partial derivatives.</p>

<div class="tip" style="
    border-left: 6px solid #000000;
    margin: 2em 2em 2em 2em;">
	<div style="background-color: #FFD1DC;
	                column-gap: 1rem;
					display: flex;
					padding: 1em 1em 1em 1em;">
	<div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #000000;">
        <i class="fas fa-external-link-alt"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">External Resources</div>
	</div>
	<div style="padding: 1em 1em 1em 1em;">
	   
<p>This Harvey Mudd College calculus tutorials explain the concept of the chain rule using dataflow diagrams.  You can view this at <a href="https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/multivariable-calculus/multi-variable-chain-rule/">HMC Multivariable Chain Rule Page</a>.</p>

<p>There is another nice writeup on this at <a href="https://math.libretexts.org/Bookshelves/Calculus/Calculus_(OpenStax)/14%3A_Differentiation_of_Functions_of_Several_Variables/14.05%3A_The_Chain_Rule_for_Multivariable_Functions">Math LibreTexts</a> (Note: that this writeup uses a slightly different graph structure where inputs that branch to multiple downstream functions are replicated)</p>

	</div>
</div>

<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 4</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p>Draw a dataflow diagram to represent the function $f(x,y,z) = \cos(x^2 y) + x^2 \sqrt{z}$.  Compute $\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}$ using the dataflow diagram method.</p>


    <button hidden="true" onclick='HideShowElement("solution-4")' class="togglebutton">Show / Hide Solution</button>

<div id="solution-4" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoiZmxvd2NoYXJ0IEJUXG5pZDFbXCIkJGYgPSBoXzMgKyBoXzV-fiQkXCJdXG5pZDJbXCIkJGhfMyA9IFxcY29zKGhfMil-fiQkXCJdXG5pZDNbXCIkJGhfNSA9IGhfMSBcXHRpbWVzIGhfNH5-JCRcIl1cbmlkNFtcIiQkaF8xID0geF4yJCRcIl1cbmlkNVtcIiQkaF80ID0gXFxzcXJ0e3p9fn4kJFwiXVxuaWQ2W1wiJCRoXzIgPSBoXzEgXFx0aW1lcyB5fn4kJFwiXVxuaWQyIC0tPiBpZDFcbmlkMyAtLT4gaWQxXG5pZDQgLS0-IGlkM1xuaWQ2IC0tPiBpZDJcbmlkNSAtLT4gaWQzXG5pZDQgLS0-IGlkNlxueCAtLT4gaWQ0XG55IC0tPiBpZDZcbnogLS0-IGlkNSIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In19"></p>

<p>
\begin{align}
\frac{\partial f}{\partial x}&amp;= \frac{\partial h_1}{\partial x} \frac{\partial h_2}{\partial h_1}  \frac{\partial h_3}{\partial h_2} \frac{\partial h_f}{\partial h_3} +  \frac{\partial h_1}{\partial x} \frac{\partial h_5}{\partial h_1} \frac{\partial f}{\partial h_5} \nonumber \\
&amp;= 2x \times y \times -\sin(h_2) \times 1 + 2x \times h_4  \times 1 \\
&amp;= -2xy \sin(x^2 y) + 2x \sqrt{z} \\
\frac{\partial f}{\partial y} &amp;= \frac{\partial h_2}{\partial y} \frac{\partial h_3}{\partial h_2} \frac{\partial f}{\partial h_3}\nonumber \\
&amp;= h_1 \times -\sin(h_2) \times 1 \\
&amp;= -x^2 \sin(x^2 y) \\ 
\frac{\partial f}{\partial z} &amp;= \frac{\partial h_4}{\partial z} \frac{\partial h_5}{\partial h_4} \frac{\partial f}{\partial h_5} \\
&amp;= \frac{1}{2} \frac{1}{\sqrt{z}} \times h_1 \times 1 \\
&amp;= \frac{1}{2} \frac{x^2}{\sqrt{z}}
\end{align}
</p>

     </div>
</div>
</div>
</div>

<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 5</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p>Come up with your own multivariable function and use a dataflow diagram to compute the partial derivative of the function with respect to each of its inputs.  If doable, sanity check your result by computing derivatives by hand.</p>


    <button hidden="true" onclick='HideShowElement("solution-5")' class="togglebutton">Show / Hide Solution</button>

<div id="solution-5" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>This is person dependent, so no solution here.  If you have a nice sample, let us know.</p>

     </div>
</div>
</div>
</div>

<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 6</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p>Suppose we have a logistic regression model with two inputs $x_1$ and $x_2$ (each of these are just scalars now) and binary outputs $y$.  Given the data flow diagram for computing the log loss of this logistic regression model, compute the partial derivative of the log loss with respect to each of its weights $w_1$ and $w_2$.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoiZmxvd2NoYXJ0IEJUXG54MVtcIiQkeF8xJCRcIl1cbngyW1wiJCR4XzIkJFwiXVxudzFbXCIkJHdfMSQkXCJdXG53MltcIiQkd18yJCRcIl1cbmgzW1wiJCRoXzMgPSBoXzEgKyBoXzIkJFwiXVxuaDFbXCIkJGhfMSA9IHhfMSB3XzEkJFwiXVxuaDJbXCIkJGhfMiA9IHhfMiB3XzIkJFwiXVxuaDRbXCIkJGhfNCA9IFxcc2lnbWEoaF8zKSQkXCJdXG5oNVtcIiQkXFxlbGwgPSAteVxcLFxcbG4oaF80KSAtICgxLXkpXFwsXFxsbigxLWhfNCl-fn5-JCRcIl1cbngxIC0tPiBoMVxudzEgLS0-IGgxXG54MiAtLT4gaDJcbncyIC0tPiBoMlxuaDEgLS0-IGgzXG5oMiAtLT4gaDNcbmgzIC0tPiBoNFxuaDQgLS0-IGg1XG55IC0tPiBoNSIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In19"></p>



    <button hidden="true" onclick='HideShowElement("solution-6")' class="togglebutton">Show / Hide Solution</button>

<div id="solution-6" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>
\begin{align}
\frac{\partial{\ell}}{w_1} &amp;= \frac{\partial h_1}{\partial w_1} \frac{\partial h_3}{\partial h_1}  \frac{\partial h_4}{\partial h_3} \frac{\partial \ell}{\partial h_4} \\
&amp;= x_1 \times 1 \times \sigma(h_3) (1-\sigma(h_3)) \times \Bigg(-y \frac{1}{\sigma(h_3)} - (1-y)\frac{1}{1-\sigma(h_3)}\Bigg) \\
&amp;= - y x_1 \sigma(h_3) (1-\sigma(h_3)) \frac{1}{\sigma(h_3)} - (1-y) x_1 \sigma(h_3) (1-\sigma(h_3)) \frac{1}{1-\sigma(h_3)} \\
&amp;= -y x_1 (1-\sigma(h_3)) - (1-y) x_1  \sigma(h_3) \\
&amp;= - x_1 (y-\sigma(h_3))~~~~~\mbox{If you plug in $y=0$ and $y=1$ you will see this is true} \\
&amp;= - x_1 (y-\sigma(w_1 x_1 + w_2 x_2))
\end{align}
</p>

     </div>
</div>
</div>
</div>


        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
  </ul>
</div>

<div class="page__footer-copyright">© 2024 Machine Learning Fall 2024 @ Olin College.</div>

      </footer>
    </div>

    <script src="/assets/js/copyCode.js"></script>


  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>










  
</body>
</html>
