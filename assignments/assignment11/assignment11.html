<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<script>
class AnchorNoProxy extends HTMLElement {
  constructor() {
    super();
    this.attachShadow({ mode: "open" });
    this._$a = null;
  }
  connectedCallback() {
    const href = this.getAttribute("href") || "#";
    if (this.dataset.hasOwnProperty('canvas')) {
        const canvasURL = this.dataset.canvas;
        this.shadowRoot.innerHTML = `<style>a:hover, a:active { outline: 0; }\na { color: #5197ad; }\na:visited { color: #5197ad; }\na:hover { color: #266477; outline: 0; }</style><a href="${href}" data-canvas="${canvasURL}"><slot></slot></a>`;
    } else {
        this.shadowRoot.innerHTML = `<style>a:hover, a:active { outline: 0; }\na { color: #5197ad; }\na:visited { color: #5197ad; }\na:hover { color: #266477; outline: 0; }</style><a href="${href}"><slot></slot></a>`;
    }
    this._$a = this.shadowRoot.querySelector("a");
    this._$a.addEventListener("click", e => {
      var url = this.getAttribute('href');
      e.preventDefault();
      if (document.referrer.startsWith('https://lms.hypothes.is') && this.dataset.hasOwnProperty('canvas')) {
        // get rid of proxy if it was added
        var n = this.dataset.canvas.search('https://olin.instructure.com');
        window.open(this.dataset.canvas.substring(n), '_blank');
      } else {
        window.open(url, '_blank');
      }
    });
  }
  static get observedAttributes() { return ["href"]; }
  attributeChangedCallback(name, oldValue, newValue) {
    if (oldValue !== newValue) {
      if (this._$a === null) return;
      this._$a.setAttribute("href", newValue);
    }
  }
}

customElements.define("a-no-proxy", AnchorNoProxy);

class NoProxy extends HTMLAnchorElement {
  connectedCallback() {
    this.addEventListener("click", e => {
      e.preventDefault();
      if (document.referrer.startsWith('https://lms.hypothes.is') && this.dataset.hasOwnProperty('canvas')) {
        // get rid of proxy if it was added
        var n = this.dataset.canvas.search('https://olin.instructure.com');
      	window.open(this.dataset.canvas.substring(n), '_blank');
      } else {
      	window.open(this.href, '_blank');
      }
    });
  }
}

customElements.define("no-proxy", NoProxy, { extends: "a" });

class ConfirmLink extends HTMLAnchorElement {
  connectedCallback() {
    this.addEventListener("click", e => {
      const result = confirm(`Are you sure you want to go to '${this.href}'?`);
      if (!result) e.preventDefault();
    });
  }
}

customElements.define("confirm-link", ConfirmLink, { extends: "a" });

</script>

<!-- begin _includes/seo.html --><title>Assignment 11 - Word Embeddings - Machine Learning Fall 2024 @ Olin College</title>
<meta name="description" content="">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Machine Learning Fall 2024 @ Olin College">
<meta property="og:title" content="Assignment 11 - Word Embeddings">
<meta property="og:url" content="/assignments/assignment11/assignment11.html">













<link rel="canonical" href="/assignments/assignment11/assignment11.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Machine Learning Fall 2024 @ Olin College Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

<script type="text/javascript">
document.addEventListener('DOMContentLoaded', function() {
    const urlParams = new URLSearchParams(window.location.search);
    const showSolutions = urlParams.get('showSolutions');
    const showAllSolutions = urlParams.get('showAllSolutions');
    const divs = document.querySelectorAll('button.togglebutton');

    if (showSolutions == 'true') {
        // Loop through the selected divs and manipulate them
        divs.forEach(div => {
            div.removeAttribute('hidden');
        });
    }

    const solutionDivs = document.querySelectorAll('[id^=solution]');
    const subpartSolutionDivs = document.querySelectorAll('[id^=subpartsolution]');

    if (showAllSolutions == 'true') {
        solutionDivs.forEach(div => {
            console.log('test')
            div.style.display = 'block';
        });

        subpartSolutionDivs.forEach(div => {
            console.log('test')
            div.style.display = 'block';
        });
    }
});
</script>

<script type="text/javascript">
function HideShowElement(divID) {
    const x = document.getElementById(divID);
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>
<style>
:root {
    --box-bg-color: #fff; /* Default background color */
}

.homework-box {
    background-color: var(--box-bg-color);
    border: 2px solid #ccc;
    padding: 15px;
    border-radius: 10px;
    margin-bottom: 20px;
    width: 300px;
}

.homework-header {
    display: flex;
    align-items: center;
    margin-bottom: 10px;
    position: relative;
}

.homework-icon {
    width: 40px;
    height: 40px;
    margin-right: 10px;
}

/* Handle missing or empty src attribute */
.homework-icon[src=""],
.homework-icon:not([src]) {
    content: url('https://upload.wikimedia.org/wikipedia/commons/a/ab/Games_for_Learning_%2827470%29_-_The_Noun_Project.svg');
    width: 40px;
    height: 40px;
    margin-right: 10px;
}

.homework-title {
    margin: 0;
    font-size: 18px;
    font-weight: bold;
}

.homework-content {
    font-size: 16px;
    color: #333;
}
</style>

<style>
.solution {
    display: none; /* Hide solutions by default */
    background-color: #f9f9f9;
    padding: 10px;
    border-left: 4px solid #007bff;
    margin-top: 10px;
}

.toggle-button {
    background-color: #007bff;
    color: #fff;
    padding: 5px 10px;
    border: none;
    border-radius: 5px;
    cursor: pointer;
    margin-top: 10px;
    display: inline-block;
}

.toggle-button.hide-solution {
    background-color: #dc3545; /* Red background for hide button */
}

img.mermaid {
     max-width:500px;
     text-align: center;
}

</style>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.7/tex-mml-chtml.js"></script>
</head>
<body>
<div style="display:none;">
$
\newcommand{\mlvec}[1]{\mathbf{#1}}
\newcommand{\mlmat}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max\,}
\DeclareMathOperator*{\argmin}{arg\,min\,}
$
</div>

  

  
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="https://qeacourse.github.io/RoboNinjaWarrior/website_graphics/olinlogo.png" alt=""></a>
        
        <a class="site-title" href="/">
          Machine Learning Fall 2024 @ Olin College
          
        </a>
        <ul class="visible-links"></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Assignment 11 - Word Embeddings">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Assignment 11 - Word Embeddings
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title">
<i class="fas fa-file-alt"></i> On this page</h4></header>
	      
                <ul class="toc__menu">
  <li><a href="#learning-objectives">Learning Objectives</a></li>
  <li><a href="#word-embeddings">Word Embeddings</a></li>
  <li><a href="#word2vec">Word2vec</a></li>
  <li><a href="#bias-in-word-embeddings">Bias in Word Embeddings</a></li>
</ul>
	      	
            </nav>
          </aside>
        
        <h1 id="learning-objectives">Learning Objectives</h1>

<div class="tip" style="
    border-left: 6px solid #000000;
    margin: 2em 2em 2em 2em;">
	<div style="background-color: #C6EBD5;
	                column-gap: 1rem;
					display: flex;
					padding: 1em 1em 1em 1em;">
	<div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #000000;">
        <i class="fas fa-brain"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Learning Objectives</div>
	</div>
	<div style="padding: 1em 1em 1em 1em;">
	   
<ul>
  <li>Learn about the concept of word embeddings and understand them as a form of unsupervised learning</li>
  <li>Understand the pros and cons of word embeddings versus the bag of words approach</li>
  <li>Examine word2vec encodings</li>
</ul>

	</div>
</div>

<h1 id="word-embeddings">Word Embeddings</h1>

<p>The concept of a word embedding was introduced in the <a href="../../activities/day13">day 13 materials</a>.  During class, we learned that a key motivation for word embeddings is to overcome a limitation that we observed with our sentiment classification algorithm from assignment 10.  Specifically, in the bag of words approach, each word is represented as an <em>independent</em> dimension in the vector that represents a particular movie review (recall that we analyzed sentiment of movie reviews in the previous assignment).</p>

<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 1</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p>Before getting into word embeddings in more detail, want to make sure you have a good handle on an important drawback of bag of words approaches.</p>

<p>Suppose, we had a training set consisting of the following movie reviews (you can assume that these are the only reviews in the training set and that we trained the model using a similar technique to what we used in assignment 10).</p>

<table>
  <thead>
    <tr>
      <th>Review</th>
      <th>Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>The casting of the movie was impeccable</td>
      <td>+</td>
    </tr>
    <tr>
      <td>The movie was great</td>
      <td>+</td>
    </tr>
    <tr>
      <td>The movie was awful</td>
      <td>-</td>
    </tr>
    <tr>
      <td>The movie was the worst I’ve ever seen</td>
      <td>-</td>
    </tr>
    <tr>
      <td>The movie was an affront to the art of film-making</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p>Explain why a bag of words model trained on this data would have a difficult time evaluating the following movie reviews from a test set.</p>

<ul>
  <li>“The movie was fantastic”</li>
  <li>“The cast of the movie did a superb job”</li>
</ul>



    <button hidden="true" onclick='HideShowElement("solution-1")' class="togglebutton">Show / Hide Solution</button>

<div id="solution-1" class="solution" style="
    background-color: #cefad0;
    border-left: 6px solid #6500B4;
    padding: 1em 1em 1em 1em;
    display: none;
    column-gap: 2rem;
    margin: 2em 2em 2em 2em;">
    <div style="display: normal">
<p style="font-weight: 900; font-size: x-large">Solution</p>

<p>For the first review, “the movie was fantastic”, the word “fantastic” does not appear in our training set.  Even though fantastic and great are closely related words, in the bag of words approach we treat each word as an independent dimension in our input vector.  If we want to understant fantastic as a synonym for great, we would need training data of movie reviews that contains the word fantastic.</p>

<p>For the second movie review, “The cast of the movie did a superb job”, even though we use many similar words to what is present in the training set, the forms of the words (e.g., the chosen tenses) prevent a match with the training set.  In order to generalize to the word forms in this movie review, we would have to have the same word forms represented in the training set.</p>

     </div>
</div>
</div>
</div>

<p><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a> were introduced as a way to overcome the issues highlighted by the previous problem.  Instead of treating each word as an independent entity, we can learn to represent (embed) each word in a vector space that preserves key properties of the words themselves.  Let’s use the symbol $r$ to represent our embedding (we’ll use $r$ since it is a <em>representation</em> of the word).  We can think of $r$ as a function from words to the vector space $\mathbb{R}^d$ (don’t get confused by this notation, $\mathbb{R}^d$ just means a d-dimensional vector of real numbers).</p>

<p>In order to learn our word embedding function $r$, we can use a form of machine learning called <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a>.  As we discussed in the previous module, unsupervised learning involves learning from unlabeled data (in contrast to the supervised learning setting we’ve been studying for most of the term where we assume we have access to a training set consisting of input / output pairs).  We can use the concept of unsupervised learning as a way to create word embeddings.  There are quite a few ways to accomplish this goal, but two foundational approaches were proposed in the paper <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space
</a>.  Here is the key figure from the paper.</p>

<figure id="figure1">
    <div style="text-align: center;">
        <img src="figures/word2vec.jpg" alt="Two choices for learning word embeddings.  On the left is the continuous bag of words (CBOW) approach where the center word is predicted from the context.  On the right is the skip gram approach where the surrounding words are predicted from the center word." style="max-width: 80%">
    </div>
    <figcaption style="font-size: large">
        Figure 1: <p>Given a sequence of words, we can pose a prediction task where we try to either predict the center word based on the embeddings of the surrounding words (CBOW) or the predict the surrounding words based on the center word (skip-gram).</p>

    </figcaption>
</figure>

<p>As mentioned in the caption for <a href="#figure1">Figure 1</a>, we can use the data itself to pose a prediction task.  You might be wondering how we can call this unsupervised learning given that we are trying to predict something (either the surrounding words or the center word).  Well, the key is that the thing we are trying to predict is derived directly from the data itself (there is no need for any additional information, or label, to be added that is not in the data already).  As such, we can use this approach to learn a word embedding from a database of text (without the need for any additional labeling).</p>

<h1 id="word2vec">Word2vec</h1>

<p>As mentioned before, word2vec was introduced in the paper <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space
</a>.  We don’t think you need to read the paper (but you are certainly welcome to!), but we do want you to get a feel the word embeddings created by word2vec.  We have put together <a href="https://colab.research.google.com/github/olinml2024/notebooks/blob/main/ML24_Assignment11.ipynb">a notebook that downloads the word embeddings and allows you to explore them a bit</a>.</p>

<h1 id="bias-in-word-embeddings">Bias in Word Embeddings</h1>

<div style="display: normal;
        border-left: 6px solid #0065B4;
        margin: 2em 0em 2em 0em;">
    <div class="tip" style="background-color: #ECF7FF;
        padding: 1em 1em 1em 1em;
        display: flex;
        column-gap: 1rem;">
        <div class="notice-bulb" style="
	    font-family: 'Font Awesome 5 Free';
	    vertical-align: middle;
	    font-weight: 900;
	    font-size: xx-large;
	    color: #ff6f00;">
        <i class="fas fa-question"></i>
</div>
	<div style="font-size: xx-large; font-weight: 900;">Exercise 2</div>
    </div>
<div style="display: normal; padding: 1em 1em 1em 1em;">
	
<p>Depending on what experiments you tried with word2vec, you may have already seen some examples of bias.  We would like you to read the paper <a href="https://www.researchgate.net/profile/Venkatesh-Saligrama/publication/305615978_Man_is_to_Computer_Programmer_as_Woman_is_to_Homemaker_Debiasing_Word_Embeddings/links/57a20cd508aeef8f311e0871/Man-is-to-Computer-Programmer-as-Woman-is-to-Homemaker-Debiasing-Word-Embeddings.pdf">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a>.  The paper gets quite technical in places, although many of the ideas you have seen before (PCA??!?).  We would like you to read sections 1-4 of the paper (sadly PCA only shows up in the later sections of the paper).  Please take notes on key takeaways and unanswered questions.  If you’d like to go into the latter sections of the paper (section 5 and beyond), please feel free to do so (this is not required, at all).</p>

<p>It’s also probably worth mentioning that the literature on bias in word embeddings is quite extensive with a lot of fascinating things to explore (and we’d love to learn from you if you if you do more explorations!).</p>


</div>
</div>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
  </ul>
</div>

<div class="page__footer-copyright">© 2024 Machine Learning Fall 2024 @ Olin College.</div>

      </footer>
    </div>

    <script src="/assets/js/copyCode.js"></script>


  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>










  
</body>
</html>
